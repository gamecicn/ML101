{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05 Training Pipeline .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdXUTeg+pa5FTmT510mCPQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 05 Training Pipeline"],"metadata":{"id":"NrWHPNVJeWpU"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQuz2DVueRzS","executionInfo":{"status":"ok","timestamp":1641527099880,"user_tz":300,"elapsed":510,"user":{"displayName":"Martin Guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16185512784682549136"}},"outputId":"0c4b52ee-b622-4b5f-e116-483a4c79c5d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["4 1\n","Prediction before training: f(5) = -4.156\n","epoch 1: w= 0.113, loss=62.40412140\n","epoch 11: w= 1.816, loss=0.05320373\n","epoch 21: w= 1.854, loss=0.03089064\n","epoch 31: w= 1.863, loss=0.02738957\n","epoch 41: w= 1.871, loss=0.02428956\n","epoch 51: w= 1.879, loss=0.02154041\n","epoch 61: w= 1.886, loss=0.01910243\n","epoch 71: w= 1.892, loss=0.01694033\n","epoch 81: w= 1.899, loss=0.01502298\n","epoch 91: w= 1.905, loss=0.01332267\n","epoch 101: w= 1.910, loss=0.01181475\n","epoch 111: w= 1.915, loss=0.01047753\n","epoch 121: w= 1.920, loss=0.00929164\n","epoch 131: w= 1.925, loss=0.00823999\n","epoch 141: w= 1.929, loss=0.00730738\n","epoch 151: w= 1.933, loss=0.00648031\n","epoch 161: w= 1.937, loss=0.00574686\n","epoch 171: w= 1.941, loss=0.00509641\n","epoch 181: w= 1.944, loss=0.00451958\n","epoch 191: w= 1.948, loss=0.00400805\n","epoch 201: w= 1.951, loss=0.00355442\n","epoch 211: w= 1.954, loss=0.00315211\n","epoch 221: w= 1.956, loss=0.00279535\n","epoch 231: w= 1.959, loss=0.00247896\n","epoch 241: w= 1.961, loss=0.00219838\n","epoch 251: w= 1.963, loss=0.00194956\n","epoch 261: w= 1.966, loss=0.00172892\n","epoch 271: w= 1.968, loss=0.00153322\n","epoch 281: w= 1.969, loss=0.00135969\n","epoch 291: w= 1.971, loss=0.00120579\n","epoch 301: w= 1.973, loss=0.00106932\n","epoch 311: w= 1.975, loss=0.00094829\n","epoch 321: w= 1.976, loss=0.00084096\n","epoch 331: w= 1.977, loss=0.00074578\n","epoch 341: w= 1.979, loss=0.00066138\n","epoch 351: w= 1.980, loss=0.00058652\n","epoch 361: w= 1.981, loss=0.00052014\n","epoch 371: w= 1.982, loss=0.00046126\n","epoch 381: w= 1.983, loss=0.00040906\n","epoch 391: w= 1.984, loss=0.00036276\n","epoch 401: w= 1.985, loss=0.00032170\n","epoch 411: w= 1.986, loss=0.00028529\n","epoch 421: w= 1.987, loss=0.00025300\n","epoch 431: w= 1.988, loss=0.00022436\n","epoch 441: w= 1.988, loss=0.00019897\n","epoch 451: w= 1.989, loss=0.00017645\n","epoch 461: w= 1.990, loss=0.00015648\n","epoch 471: w= 1.990, loss=0.00013877\n","epoch 481: w= 1.991, loss=0.00012306\n","epoch 491: w= 1.991, loss=0.00010913\n","epoch 501: w= 1.992, loss=0.00009678\n","epoch 511: w= 1.992, loss=0.00008583\n","epoch 521: w= 1.993, loss=0.00007611\n","epoch 531: w= 1.993, loss=0.00006750\n","epoch 541: w= 1.994, loss=0.00005986\n","epoch 551: w= 1.994, loss=0.00005308\n","epoch 561: w= 1.994, loss=0.00004708\n","epoch 571: w= 1.995, loss=0.00004175\n","epoch 581: w= 1.995, loss=0.00003702\n","epoch 591: w= 1.995, loss=0.00003283\n","epoch 601: w= 1.996, loss=0.00002912\n","epoch 611: w= 1.996, loss=0.00002582\n","epoch 621: w= 1.996, loss=0.00002290\n","epoch 631: w= 1.996, loss=0.00002031\n","epoch 641: w= 1.996, loss=0.00001801\n","epoch 651: w= 1.997, loss=0.00001597\n","epoch 661: w= 1.997, loss=0.00001416\n","epoch 671: w= 1.997, loss=0.00001256\n","epoch 681: w= 1.997, loss=0.00001114\n","epoch 691: w= 1.997, loss=0.00000988\n","epoch 701: w= 1.998, loss=0.00000876\n","epoch 711: w= 1.998, loss=0.00000777\n","epoch 721: w= 1.998, loss=0.00000689\n","epoch 731: w= 1.998, loss=0.00000611\n","epoch 741: w= 1.998, loss=0.00000542\n","epoch 751: w= 1.998, loss=0.00000480\n","epoch 761: w= 1.998, loss=0.00000426\n","epoch 771: w= 1.998, loss=0.00000378\n","epoch 781: w= 1.998, loss=0.00000335\n","epoch 791: w= 1.999, loss=0.00000297\n","epoch 801: w= 1.999, loss=0.00000264\n","epoch 811: w= 1.999, loss=0.00000234\n","epoch 821: w= 1.999, loss=0.00000207\n","epoch 831: w= 1.999, loss=0.00000184\n","epoch 841: w= 1.999, loss=0.00000163\n","epoch 851: w= 1.999, loss=0.00000145\n","epoch 861: w= 1.999, loss=0.00000128\n","epoch 871: w= 1.999, loss=0.00000114\n","epoch 881: w= 1.999, loss=0.00000101\n","epoch 891: w= 1.999, loss=0.00000089\n","epoch 901: w= 1.999, loss=0.00000079\n","epoch 911: w= 1.999, loss=0.00000070\n","epoch 921: w= 1.999, loss=0.00000062\n","epoch 931: w= 1.999, loss=0.00000055\n","epoch 941: w= 1.999, loss=0.00000049\n","epoch 951: w= 1.999, loss=0.00000043\n","epoch 961: w= 1.999, loss=0.00000039\n","epoch 971: w= 2.000, loss=0.00000034\n","epoch 981: w= 2.000, loss=0.00000030\n","epoch 991: w= 2.000, loss=0.00000027\n","Prediction after training: f(5) = 9.999\n"]}],"source":["# 1 Design model \n","# 2 Construct loss and optimizer\n","# 3 Training loop\n","#    -- forward pass: compute predition\n","#    -- backward pass: gradients\n","#    -- update weight\n","\n","import torch \n","import torch.nn as nn\n","\n","\n","X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","\n","''' \n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","# Traget function\n","def forward(x):\n","  return w * x\n","'''\n","X_test = torch.tensor([5], dtype=torch.float32)\n","\n","n_sample, n_feature = X.shape\n","print(n_sample, n_feature)\n","\n","input_size = n_feature\n","output_size = n_feature\n","\n","\n","class LinearRegression(nn.Module):\n","\n","  def __init__(self, input_dim, output_dim):\n","    super(LinearRegression, self).__init__()\n","\n","    #define layer\n","    self.lin = nn.Linear(input_dim, output_dim)\n","\n","  def forward(self, x):\n","    return self.lin(x)\n","\n","\n","model = LinearRegression(input_size, output_size)\n","\n","\n","\n","\n","print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n","\n","learning_rate = 0.02\n","n_iters = 1000\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","\n","  # prediction = forward pass\n","  y_pred = model(X)\n","\n","  # loss\n","  l = loss(Y, y_pred)\n","\n","  # gradients = backward pass\n","  l.backward() #dl/dw\n","\n","  # update weights\n","  optimizer.step()\n","\n","  # zero gradients\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    [w, b] = model.parameters()\n","    print(f'epoch {epoch + 1}: w= {w[0][0]:.3f}, loss={l:.8f}')\n","\n","\n","print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")\n","\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"8PLGHounoDsI"},"execution_count":null,"outputs":[]}]}